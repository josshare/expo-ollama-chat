# ğŸ¦™ Expo Ollama Chat

A production-ready Expo app that runs a fully offline LLM using Ollama in Docker. Chat with AI models running locally on your machine - no complex setup required!

## ğŸš€ One-Command Setup

```bash
npm start
```

That's it! This single command will:
- âœ… Start Ollama in Docker automatically
- âœ… Install all dependencies  
- âœ… Download a fast AI model (llama3.2:1b)
- âœ… Start the Expo development server

## ğŸ“‹ Prerequisites

You only need **Docker** installed on your system:

### Install Docker:
```bash
# Visit: https://docs.docker.com/get-docker/
# Docker Desktop includes Docker Compose
```

That's all! No need to install Ollama separately - it runs in Docker.

## ğŸ¯ Features

- ğŸ”¥ **Fully Offline** - No API keys or internet required
- âš¡ **Fast Setup** - One command to run everything
- ğŸ¤– **Smart AI** - Uses Llama 3.2 1B model (1.3GB)
- ğŸ“± **Native Feel** - Smooth chat UI with React Native
- ğŸ”’ **Private** - All conversations stay on your device
- ğŸ¨ **Beautiful UI** - Modern chat interface
- ğŸ“± **Cross Platform** - iOS, Android, Web support

## ğŸ›  Manual Setup (if needed)

If the automatic setup fails:

```bash
# 1. Start Ollama in Docker
docker-compose up -d

# 2. Pull a model
docker exec ollama-server ollama pull llama3.2:1b

# 3. Install dependencies
npm install

# 4. Start Expo
npx expo start --clear
```

## ğŸ¤– Using Different Models

You can use any Ollama-compatible model:

```bash
# Larger models (better quality, slower)
docker exec ollama-server ollama pull llama3.2:3b
docker exec ollama-server ollama pull mistral
docker exec ollama-server ollama pull codellama

# Smaller models (faster, less capable)  
docker exec ollama-server ollama pull phi3:mini
docker exec ollama-server ollama pull tinyllama

# List available models
docker exec ollama-server ollama list
```

The app will automatically detect and use available models.

## ğŸ“± Running the App

### Development:
```bash
npm run dev           # Start Expo dev server
npm run ios           # Run on iOS simulator
npm run android       # Run on Android emulator
npm run web           # Run in web browser
```

### Scripts:
```bash
npm run setup         # Full setup (check Ollama + install + model)
npm run check-ollama  # Check if Ollama is running
npm run setup-model   # Download AI model
```

## ğŸ”§ Troubleshooting

### "Ollama not running" error:
```bash
docker-compose up -d
```

### "Model not found" error:
```bash
docker exec ollama-server ollama pull llama3.2:1b
docker exec ollama-server ollama list
```

### Connection issues:
- Make sure Docker is running
- Check if container is up: `docker ps`
- Try restarting: `docker-compose restart`
- View logs: `docker logs ollama-server`

### Performance tips:
- Use smaller models for faster responses: `phi3:mini`, `tinyllama`
- Close other applications to free up RAM
- Use SSD storage for better model loading

## ğŸ— Project Structure

```
â”œâ”€â”€ App.tsx                 # Main React Native app
â”œâ”€â”€ services/
â”‚   â””â”€â”€ OllamaService.ts   # Ollama API client
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ check-ollama.js    # Ollama installation check
â”‚   â””â”€â”€ setup-model.js     # Model download script
â”œâ”€â”€ docker-compose.yml     # Docker configuration
â”œâ”€â”€ start.sh              # One-command startup script
â””â”€â”€ package.json          # Dependencies & scripts
```

## ğŸ”’ Privacy & Security

- ğŸ›¡ **100% Local** - No data leaves your device
- ğŸš« **No Tracking** - No analytics or telemetry  
- ğŸ” **No API Keys** - No external services
- ğŸ’¾ **Local Storage** - Conversations stored locally

## ğŸ“Š System Requirements

- **RAM**: 4GB+ (8GB+ recommended)
- **Storage**: 2GB+ free space
- **OS**: macOS, Linux, Windows  
- **Node.js**: 18+
- **Expo CLI**: Latest

## ğŸš€ Model Performance

| Model | Size | Speed | Quality | Memory |
|-------|------|-------|---------|--------|
| llama3.2:1b | 1.3GB | âš¡âš¡âš¡ | â­â­â­ | 2GB |
| llama3.2:3b | 2.0GB | âš¡âš¡ | â­â­â­â­ | 4GB |
| mistral | 4.1GB | âš¡ | â­â­â­â­â­ | 6GB |

## ğŸ¤ Contributing

This is a complete, production-ready example. Feel free to:
- Add new features
- Improve the UI
- Support more models  
- Add voice input/output
- Create themes

## ğŸ“„ License

MIT License - Use this code however you want!

---

**Built with â¤ï¸ using Expo + Ollama**

*Chat with AI, completely offline, right from your phone!*